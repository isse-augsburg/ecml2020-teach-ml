{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBYLzqrlejbL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons, TextBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in 1D\n",
    "This exercise illustrates linear regression, its solution in closed form and by means of gradient descent.\n",
    "Use only the following cell (i.e. the class `LinearRegressor`) for this task, the connection to the GUI is already set for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "ue6wxinveNRY",
    "outputId": "4e2948c6-760a-41d9-e396-aab4afef2f70"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "class LinearRegressor:\n",
    "  \"\"\"\n",
    "  This class implements a linear regression model, has its own\n",
    "  pointers to a 1D training set (train_X) and associated labels (train_t)\n",
    "\n",
    "  For instance:\n",
    "  train_X   |    train_t\n",
    "  ----------------------\n",
    "   0.6      |        7.9\n",
    "   1.9      |        4.2\n",
    "  -0.3      |       12.9\n",
    "  ----------------------\n",
    "  train_X and train_t are of shape (n_training_examples,)\n",
    "\n",
    "  The model is specified as:\n",
    "  y_pred = w0 + w1 * x\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, w0, w1, train_X, train_t):\n",
    "    \"\"\"\n",
    "    The constructor sets some important values\n",
    "    :param w0: initial offset (bias)\n",
    "    :param w1: initial coefficient for x\n",
    "    :param train_X: training samples inputs\n",
    "    :param train_t: training samples targets\n",
    "    \"\"\"\n",
    "    self.w0 = w0\n",
    "    self.w1 = w1\n",
    "    self.train_X = train_X\n",
    "    self.train_t = train_t\n",
    "    self.eps = 0.01\n",
    "    self.max_iterations = 500\n",
    "\n",
    "  def predict(self, x, w0, w1):\n",
    "    \"\"\"\n",
    "    Evaluates y_pred = w0 + w1 * x with the given parameters\n",
    "    :param x: input\n",
    "    :param w0: offset (bias)\n",
    "    :param w1: weight for x\n",
    "    :return: the model's output\n",
    "    \"\"\"\n",
    "    return w0 + w1 * x\n",
    "\n",
    "  def get_predictions(self, x):\n",
    "    return self.predict(x, self.w0, self.w1)\n",
    "\n",
    "  def get_loss(self):\n",
    "    \"\"\"\n",
    "    Calculates the loss with the current parameter settings (w0, w1) and the training set\n",
    "    :return: mean squared error (MSE)\n",
    "    \"\"\"\n",
    "    # Task a)\n",
    "    ## TODO Implement the mean squared error (MSE) for the current values of w0 and w1 \n",
    "    ## TODO and the training quantities (train_X, train_t).\n",
    "    # train_t is given and has to be compared against y_pred\n",
    "    \n",
    "    y_pred = self.get_predictions(self.train_X)\n",
    "    loss = 0\n",
    "    \n",
    "    # ----- Solution -----\n",
    "    diff = y_pred - self.train_t\n",
    "    summed_error = np.dot(diff, diff)\n",
    "    loss = summed_error / len(self.train_t)\n",
    "    # --------------------\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "  def solve_normal_equation(self):\n",
    "    \"\"\"\n",
    "    Calculates the optimal solution in terms of mean squared error\n",
    "    according to the training set using the closed form solution (= normal equations)\n",
    "    :return: a tuple (w0, w1) for the optimal weights\n",
    "    \"\"\"\n",
    "    # Task d)\n",
    "    ## TODO Implement the normal equations and set values for w0 and w1.\n",
    "    self.w0, self.w1 = 0, 0\n",
    "    \n",
    "    # ----- Solution -----\n",
    "    # optimizer used \n",
    "    regr = linear_model.LinearRegression(normalize=False)\n",
    "    regr.fit(train_x.reshape(-1, 1), train_t)\n",
    "    self.w0, self.w1 = regr.intercept_, regr.coef_\n",
    "    # --------------------\n",
    "    \n",
    "    return self.w0, self.w1\n",
    "\n",
    "  def gradient(self):\n",
    "    \"\"\"\n",
    "    calculates the derivatives d Loss / d w0 and d Loss / d w1\n",
    "    :return:the analytical loss gradient as a tuple (for w0 and w1)\n",
    "    \"\"\"\n",
    "    # Task b)\n",
    "    ## TODO Calculate the gradient first on paper and implement your formulas afterwards.\n",
    "    ## TODO Verify the correctness of your formulas with the numerical gradient check, \n",
    "    ## TODO wher you estimate the local gradient in a small region arount the desired \n",
    "    ## parameter values, e.g. (L(w0 + eps, w1) - L(w0, w1))/ eps    for a small eps.\n",
    "    \n",
    "    w0_grad, w1_grad = 0.0, 0.0\n",
    "    \n",
    "    # ----- Solution -----\n",
    "    y_pred = self.predict(self.train_X, self.w0, self.w1)\n",
    "    diff = (y_pred - self.train_t)\n",
    "    w0_grad = (2. * np.sum(diff) / len(y_pred))\n",
    "    gradSum = np.dot(diff, self.train_X)\n",
    "    w1_grad = 2. * gradSum / len(y_pred)    \n",
    "    # --------------------\n",
    "    \n",
    "    return w0_grad, w1_grad\n",
    "\n",
    "  def step_gradient(self, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a parameter update according to the gradient calculated in self.gradient()\n",
    "    :param learning_rate:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    g_w0, g_w1 = self.gradient()\n",
    "    self.w0, self.w1 = self.w0 - learning_rate*g_w0, self.w1 - learning_rate * g_w1\n",
    "\n",
    "  def solve_gradient_descent(self, learning_rate):\n",
    "    \"\"\"\n",
    "    Calculates the optimal solution in terms of mean squared error\n",
    "    according to the training set using gradient descent\n",
    "    :return: a tuple w0, w1 for the optimal settings\n",
    "    \"\"\"\n",
    "    # Task c)\n",
    "    ## TODO Implement gradient descent using self.step_gradient.\n",
    "    ## TODO Choose a meaningful termination criterion to assure your gradient method terminates.\n",
    "    ## TODO As simple example, you could use over a fixed number of iterations\n",
    "    \n",
    "    # ----- Solution -----\n",
    "    for i in range(self.max_iterations):\n",
    "        old_w0, old_w1 = self.w0, self.w1\n",
    "        self.step_gradient(learning_rate)\n",
    "        # convergence criterion, if we do not need all iterations\n",
    "        if np.abs(old_w0 - self.w0) < self.eps and np.abs(old_w1 - self.w1) < self.eps:\n",
    "            break\n",
    "    # --------------------\n",
    "    \n",
    "    return self.w0, self.w1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT TOUCH THIS CODE!\n",
    "\n",
    "# some mild formatting\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "orange = (1.0, 0.57647, 0.039216)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "axes : plt.Axes = ax\n",
    "\n",
    "fig.set_size_inches(10,4.5)\n",
    "plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "\n",
    "# our input domain\n",
    "x_min = 0.0\n",
    "x_max = 10.0\n",
    "\n",
    "x = np.arange(x_min, x_max, 0.01)\n",
    "# the parameters that can be tuned by users\n",
    "w0 = -4.\n",
    "w1 = 1.2\n",
    "use_noise = True\n",
    "lock_update = False\n",
    "\n",
    "def my_func(w0, w1, x):\n",
    "    return w0 + w1*x\n",
    "\n",
    "\n",
    "f = my_func(w0, w1, x)\n",
    "l, = plt.plot(x, f, lw=2, color=orange, label = \"Prediction\" )\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"y = w0 + w1 * x\")\n",
    "\n",
    "# now for some ground truth test samples\n",
    "np.random.seed(137)\n",
    "n_train = 8\n",
    "w0_true = 6.2\n",
    "w1_true = -1.78\n",
    "\n",
    "train_x, train_t = (0,0)\n",
    "\n",
    "\n",
    "def recreate_training_set():\n",
    "    global train_x\n",
    "    global train_t\n",
    "    np.random.seed(1337)\n",
    "    train_x = np.random.rand(n_train) * x_max\n",
    "    train_t = my_func(w0_true, w1_true, train_x)\n",
    "\n",
    "    if use_noise:\n",
    "        noise = np.random.normal(0, 1, n_train)\n",
    "        train_t = train_t + noise\n",
    "\n",
    "\n",
    "recreate_training_set()\n",
    "\n",
    "regressor = LinearRegressor(w0, w1, train_x, train_t)\n",
    "\n",
    "train_plot, = plt.plot(train_x, train_t, 'xb', label=\"Train set\")\n",
    "vlines = ax.vlines(train_x, my_func(w0, w1, train_x), train_t)\n",
    "\n",
    "plt.axis([0, x_max, -11, 11])\n",
    "\n",
    "\n",
    "axcolor = 'lightgoldenrodyellow'\n",
    "axloss = plt.axes([0.25, 0.9, 0.65, 0.06])\n",
    "axfreq = plt.axes([0.25, 0.1, 0.65, 0.03], facecolor=axcolor)\n",
    "axamp = plt.axes([0.25, 0.15, 0.65, 0.03], facecolor=axcolor)\n",
    "\n",
    "lossLabel = TextBox(axloss,\"Loss (MSE)\", \"0.0\")\n",
    "sw0 = Slider(axamp, 'w0', -15.0, 15.0, valinit=w0)\n",
    "sw1 = Slider(axfreq, 'w1', -50.0, 50.0, valinit=w1)\n",
    "\n",
    "def update_with_params(w0, w1):\n",
    "    l.set_ydata(regressor.predict (x, w0, w1))\n",
    "    train_plot.set_ydata(train_t)\n",
    "\n",
    "    train_y_pred = regressor.predict (train_x, w0, w1)\n",
    "    \n",
    "    train_t_points = np.vstack((train_x, train_t)).T\n",
    "    train_y_points = np.vstack((train_x, train_y_pred)).T\n",
    "    segs_ = np.hstack((train_t_points, train_y_points))\n",
    "    segs = segs_.reshape(len(train_t),2,2)\n",
    "\n",
    "    vlines.set_segments(segs)\n",
    "    # calculate new loss\n",
    "    y_pred = my_func(w0, w1, train_x)\n",
    "    loss = regressor.get_loss()\n",
    "    lossLabel.set_val(str(loss))\n",
    "\n",
    "    # calculate and show gradients\n",
    "    grad_w0, grad_w1 = regressor.gradient()\n",
    "    loss_label_grad_w0.set_val(np.around(grad_w0,2))\n",
    "    loss_label_grad_w1.set_val(np.around(grad_w1,2))\n",
    "\n",
    "\n",
    "def update(val):\n",
    "    if not lock_update:\n",
    "        w0 = sw0.val\n",
    "        w1 = sw1.val\n",
    "        regressor.w0, regressor.w1 = w0, w1\n",
    "        update_with_params(w0, w1)\n",
    "\n",
    "\n",
    "sw0.on_changed(update)\n",
    "sw1.on_changed(update)\n",
    "\n",
    "curr_width = 0.82\n",
    "resetax = plt.axes([curr_width , 0.01, 0.08, 0.06])\n",
    "curr_width -= 0.16\n",
    "toggle_gt_ax = plt.axes([curr_width, 0.01, 0.13, 0.06])\n",
    "curr_width -= 0.28\n",
    "solve_NE_ax = plt.axes([curr_width, 0.01, 0.25, 0.06])\n",
    "curr_width -= 0.28\n",
    "solve_GD_ax = plt.axes([curr_width, 0.01, 0.25, 0.06])\n",
    "\n",
    "# buttons\n",
    "reset_button = Button(resetax, 'Reset', color=axcolor, hovercolor='0.975')\n",
    "toggle_secret_button = Button(toggle_gt_ax, 'Show secret', color=axcolor, hovercolor='0.975')\n",
    "solve_NE_button = Button(solve_NE_ax, 'Solve by normal equations', color=axcolor, hovercolor='0.975')\n",
    "solve_GD_button = Button(solve_GD_ax, 'Solve by gradient descent', color=axcolor, hovercolor='0.975')\n",
    "\n",
    "def reset(event):\n",
    "    sw0.reset()\n",
    "    sw1.reset()\n",
    "\n",
    "def toggle_gt(event):\n",
    "    ground_truth_ax_w0.set_visible(not (ground_truth_ax_w0.get_visible()) )\n",
    "    ground_truth_ax_w1.set_visible(not (ground_truth_ax_w1.get_visible()) )\n",
    "\n",
    "def solve_normal_equation(event):\n",
    "    global  lock_update\n",
    "    w0, w1 = regressor.solve_normal_equation()\n",
    "    lock_update = True\n",
    "    sw0.set_val(w0)\n",
    "    sw1.set_val(w1)\n",
    "    lock_update = False\n",
    "    update_with_params(w0, w1)\n",
    "\n",
    "reset_button.on_clicked(reset)\n",
    "solve_NE_button.on_clicked(solve_normal_equation)\n",
    "toggle_secret_button.on_clicked(toggle_gt)\n",
    "\n",
    "noise_switch_ax = plt.axes([0.00, 0.7, 0.15, 0.15], facecolor=axcolor)\n",
    "noise_switch = RadioButtons(noise_switch_ax, ('Noise on', 'Noise off'), active=0)\n",
    "\n",
    "\n",
    "ax_learning_rate = plt.axes([0.12, 0.57, 0.05, 0.05])\n",
    "input_learning_rate = TextBox(ax_learning_rate, \"Learning rate \", \"0.01\", color=axcolor)\n",
    "\n",
    "# text boxes for grad w0 and grad w1\n",
    "current_line = 0.45\n",
    "ax_loss_grad_w0_label = plt.axes([0.12, current_line, 0.05, 0.05])\n",
    "current_line -= 0.05\n",
    "ax_loss_grad_w1_label = plt.axes([0.12, current_line, 0.05, 0.05])\n",
    "current_line -= 0.21\n",
    "grad_step_button_ax = plt.axes([0.00, current_line, 0.15, 0.06])\n",
    "\n",
    "loss_label_grad_w0 = TextBox(ax_loss_grad_w0_label, \"d Loss / d w0 \", \"0.0\")\n",
    "loss_label_grad_w1 = TextBox(ax_loss_grad_w1_label, \"d Loss / d w1 \", \"0.0\")\n",
    "\n",
    "\n",
    "grad_button = Button(grad_step_button_ax, 'Gradient step', color=axcolor, hovercolor='0.975')\n",
    "\n",
    "# the \"secret\", i.e., the true parameters used to generate the training data\n",
    "ground_truth_ax_w0 = plt.axes([0.07, 0.33, 0.05, 0.05], facecolor=axcolor)\n",
    "ground_truth_label_w0 = TextBox(ground_truth_ax_w0, \"w0 \", str(w0_true))\n",
    "ground_truth_ax_w0.set_visible(False)\n",
    "\n",
    "ground_truth_ax_w1 = plt.axes([0.07, 0.28, 0.05, 0.05], facecolor=axcolor)\n",
    "ground_truth_label_w1 = TextBox(ground_truth_ax_w1, \"w1 \", str(w1_true))\n",
    "ground_truth_ax_w1.set_visible(False)\n",
    "\n",
    "def take_gradient_step(event):\n",
    "  learning_rate = float(input_learning_rate.text)\n",
    "  regressor.step_gradient(learning_rate)\n",
    "  update_with_params(regressor.w0, regressor.w1)\n",
    "  return\n",
    "grad_button.on_clicked(take_gradient_step)\n",
    "\n",
    "def solve_gradient_descent(event):\n",
    "  global lock_update\n",
    "  learning_rate = float(input_learning_rate.text)\n",
    "  w0, w1 = regressor.solve_gradient_descent(learning_rate)\n",
    "  lock_update = True\n",
    "  sw0.set_val(w0)\n",
    "  sw1.set_val(w1)\n",
    "  lock_update = False\n",
    "  update_with_params(w0, w1)\n",
    "\n",
    "solve_GD_button.on_clicked(solve_gradient_descent)\n",
    "\n",
    "def noise_switch_func(label):\n",
    "    global use_noise\n",
    "    use_noise = (label == 'Noise on')\n",
    "    recreate_training_set()\n",
    "    regressor.train_X = train_x\n",
    "    regressor.train_t = train_t\n",
    "    update(None)\n",
    "    return\n",
    "\n",
    "noise_switch.on_clicked(noise_switch_func)\n",
    "ax.legend(loc=2)\n",
    "update_with_params(w0, w1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
